Topics in Philosophy of Psychology <small>with Professor Frances Egan</small>
=============================================================================

January 28th, 2013 <small>Reading</small>
-----------------------------------------

### *Computation and Cognition* by Zenon W. Pylyshyn, Chapter 2, "The Explanatory Role of Representation"

#### Introduction

-   The hardest puzzle is consciousness.
    -   Second hardest is *meaning*, which this work explains.
    -   Does *not* solve the puzzle of meaning.
    -   The author aims to **describe how the idea of the semantic
        content of representations is implicitly viewed within the field
        of cognitive science, and discuss why this view is
        justifiable**.

Representations
:   Generalizations stated over the contents of representations are not
    mere functional generalization in the usual sense.

Function generalizations
:   A theory that does not refer to physical properties of the
    particular system in question, only how it operates.

-   There will be a *representational level* and a *symbol-processing
    level*.

#### The Appeal to Representations

-   Law-like generalization and explanations can differ in several ways,
    consider:
    1.  A certain object accelerated at *a* meters per second per second
        because a steady force was applied that was equal to *ma*.
    2.  A certain neuron fired because a potential of *v* millivolts was
        applied along two of its sentries and that it had been inactive
        during the previous *t* milliseconds
    3.  A bit pattern of certain computer register came to have a
        particular configuration because of the particular contents
        present in the instruction register and the program counter, and
        because the system is wired according to a certain transfer
        protocol.
    4.  The computer printed numbers 2, 4, 6, because it started with
        the number 2 and added 2 repeatedly or because it applied the
        successor function repeatedly and double the value before
        printing.
    5.  The pedestrian dialed 911 because he believed it to be the
        emergency number and had recognized the urgent need for
        assistance.

-   Accounts (1), (2), and (3), all the terms refer to properties of
    objects within the closed system.[^1]
-   Accounts (4) and (6) are different in this important respect: Both
    make substantive reference to entities or properties that are not an
    intrinsic part of their state description, that is *numbers* and
    *need for assistance*.

> How is it possible for properties of the world to determine behavior
> when the properties are not causally related in the required sense to
> to the functional states of the system? --- **Brentano's problem**

-   The notion of representation is necessary only in the context of
    explanation.
-   Behavior is being caused by certain states of one's brain, and so
    mental states themselves are related to agent's actions.
-   Brain states are not causally connected in appropriate ways to
    walking or mountains.
    -   The relationship *is one of content*, a semantic, not causal,
        relationship.
        -   The notion of content is roughly that of what the states
            *are about*.

    -   Brain states cause certain movements. If these movements are
        view as members of equivalence classes described as "writing a
        sentence about walking in the Santa Cruz mountains" the brains
        states must be treated as embodying representations of these
        codes by certain rules.

-   Contrast the brain to a watch -- a watch's "behavior" is considered
    coextensive with the set of movements corresponding to the physical
    description of behavior.
    -   Two ways of explaining human behavior capture extremely
        different generalizations.

#### Representational and Functional Levels

-   This shows that **a *functional* description of mental processes is
    not enough, there must also be content**.

> If the content makes a difference to behavior, is it not also a
> functional difference?

-   To be in a certain representational state is to have a certain
    symbolic expression in some part of memory.
    -   The expression *encodes* the semantic interpretation and the
        combinatorial structure of the expression encodes the relation
        among the contents of the subexpressions, much as in the
        combinatorial system of predicate calculus.

-   The reason there must be symbolic codes is that they can enter in
    causal relations.[^2]
-   If there is a unique symbolic expression corresponding to each
    content, one might expect functional states and representational
    states to once again be one-to-one relation. Not so, because:
    1.  There may be codes with the same semantic content which are
        functionally but not semantically distinguishable.
    2.  Merely possessing a certain symbolic expression that encodes
        semantic content is insufficient to produce behavior.
        -   You need to *interpret* the symbols.

Semantic-level generalization
:   Generalizations expressible in terms of the semantic content of
    representations.

:   Newell calls this "knowledge-level."

Symbol-level generalizations
:   Generalizations expressible in terms of functional properties of the
    functional architecture.

#### Representational Content as Defining a Level of Description

-   We abandoned a biological vocabulary because of arbitrarily large
    disjunctions corresponding to processes like "thinking."
    -   Functional generalizations cannot be captured in a finite
        neurophysiological description.
    -   There is a vocabulary in between "*n* fired at *t* with *v*" and
        "He called 911 because he believed he was in an emergency."
        -   And it's *functional*.
        -   And it's in *semantic terms*.

-   "The principal of rationality is a major reason for our belief that
    a purely functional account will fail to capture certain
    generalizations, hence, that a distinct new level is required."

##### Levels and Constraints on Realizability

-   For a description, that description might be compatible with other
    levels.
    -   Newtons laws "are compatible with" biological taxonomy.

[^1]: I don't understand when you're "supposed to stop describing" for a
    closed system. I see that account (4) stops describing when the
    internal state of the computer comes to be required to continue
    explaining, but I don't understand why in account (3), supposedly
    "closed", you don't say that bits come to be charged via
    electricity, for instance.

    I think that even account (1), (2), and (3) really require an
    arbitrarily large conjunction of state description to be "closed."

[^2]: I want to know what a biological or neurological taxonomy of types
    would look like -- our ability to type artifacts and concepts seems
    sufficiently general that "anything can fit in the bin", that is we
    can store tokens of mountains and transcendental idealism -- if
    token couldn't be contained in any of our brain's possible types,
    would we be in a position to know?
